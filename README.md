# ğŸ§  LLM from Scratch

This project implements a transformer-based language model **from scratch** â€” no `torch.nn.Linear`, no `torch.optim.Adam`, no `torch.nn.transformer`, no `torch.nn.CrossEntropyLoss`. Every core component of the model architecture and training pipeline is manually built using low-level PyTorch.

---

## âœ¨ Features

- âœ… Custom `Linear` and `Embedding` layers, `softmax`,`cross_entropy_loss`, `gradient clipping` and many more
- âœ… Custom `AdamW` optimizer with cosine learning rate scheduler
- âœ… Full Transformer block with:
  - Multi-head self-attention
  - Rotary positional embeddings (RoPE)
  - RMSNorm
  - SwiGLU feedforward network
- âœ… Tokenization with BPE + `np.memmap` streaming
- âœ… Autoregressive decoding with top-p sampling
- âœ… Integrated Weights & Biases (W&B) logging

---

## ğŸ“ Project Structure

```
your_project/
â”œâ”€â”€ scripts/              # Training and decoding entry points
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ decode.py
|
â”œâ”€â”€ models/               # Transformer model & layers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ layers.py
â”‚   â””â”€â”€ attention.py
|   â””â”€â”€ tokenizer.py
|   â””â”€â”€ transformer.py
â”œâ”€â”€ utilities/                # Data loading, optimization, training utils
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_utils.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ training.py
â”œâ”€â”€ optim/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ adamw.py
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ train_config.yaml
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ transformer_checkpoint.pt
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. Tokenization

You can either any tokenizer from HuggingFace. But I implemented my own BPE which is super faster, because I optimized using parallization and caching. Refer https://github.com/bargav25/fast_bpe

Follow that and it tokenizes your train and validation text into token_ids and store them to `.memmap` files


### 2. Train the model

```bash
python scripts/train.py --config configs/train_config.yaml --use_wandb
```

(If you're on a cluster, donâ€™t forget to export your W&B API key first.)

### 3. Decode (generate text)

```bash
python scripts/decode.py
```

---

I trained my model on Tiny Stories dataset (~2.12 million documents) which took me around 2-3 hours on A100, and the results were pretty great.

![loss_curves](images/loss_curves.png "Loss Curves")


## ğŸ§ª Training Tips

- Use `np.memmap` for memory-efficient token loading
- Monitor `val_loss` in W&B to check overfitting
- Adjust `d_model`, `num_layers`, or `context_length` for capacity
- Use `<|endoftext|>` as a natural stopping token in decoding

---

## ğŸ“Š Sample Weights & Biases Integration

Enable with:
```bash
wandb login
python scripts/train.py --config configs/train_config.yaml --use_wandb
```

Track:
- ğŸ“‰ Training & validation loss
- ğŸ” Learning rate schedule
- ğŸ“¦ Checkpoint intervals
- ğŸ§  Gradient norms

